{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myeze/WhisperBuildTrimmer/blob/main/PyannoteWhisperTrimmer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQyROdrfsvk4"
      },
      "source": [
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/Majdoddin/nlp/blob/main/Pyannote_plays_and_Whisper_rhymes_v_2_0.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/majdoddin/nlp)\n",
        "\n",
        "# Whisper's transcription plus Pyannote's Diarization\n",
        "\n",
        "**This notebook contains changes made by Myles Ezeanii.**\n",
        "\n",
        "These changes include:\n",
        "- Interval based console output (Write audio to the console by second or by speaker)\n",
        "- The ability to trim audio (Only use a certain section of audio)\n",
        "- The ability to use recorded audio\n",
        "\n",
        "More changes will be added per needed. The additional/previous authors are listed below:\n",
        "\n",
        "Original Discussion -\n",
        "https://github.com/openai/whisper/discussions/264\n",
        "\n",
        "---\n",
        "\n",
        "**Update** - [@johnwyles](https://github.com/johnwyles) added HTML output for audio/video files from Google Drive, along with some fixes.\n",
        "\n",
        "Using the new word-level timestamping of Whisper, the transcription words are highlighted as the video plays, with optional autoscroll. And the display on small displays is improved.\n",
        "\n",
        "Moreover, the model is loaded just once, thus the whole thing runs much faster now. You can also hardcode your Huggingface token.\n",
        "\n",
        "---\n",
        "Andrej Karpathy [suggested](https://twitter.com/karpathy/status/1574476200801538048?s=20&t=s5IMMXOYjBI6-91dib6w8g) training a classifier on top of  OpenAI [Whisper](https://openai.com/blog/whisper/) model features to identify the speaker, so we can visualize the speaker in the transcript. But, as [pointed out](https://twitter.com/tarantulae/status/1574493613362388992?s=20&t=s5IMMXOYjBI6-91dib6w8g) by Christian Perone, it seems that features from whisper wouldn't be that great for speaker recognition as its main objective is basically to ignore speaker differences.\n",
        "\n",
        "In the following, I use [**`pyannote-audio`**](https://github.com/pyannote/pyannote-audio), a speaker diarization toolkit by Herv√© Bredin, to identify the speakers, and then match it with the transcriptions of Whispr, linked to the video. The input can be YouTube or an video/audio file (also on Google Drive). I try it on a [Customer Support Call](https://youtu.be/hpZFJctBUHQ). Check the result [**here**](https://majdoddin.github.io/dyson.html).\n",
        "\n",
        "To make it easier to match the transcriptions to diarizations by speaker change, Sarah Kaiser [suggested](https://github.com/openai/whisper/discussions/264#discussioncomment-3825375) runnnig the pyannote.audio first and  then just running whisper on the split-by-speaker chunks.\n",
        "For sake of performance (and transcription quality?), we attach the audio segments into a single audio file with a silent spacer as a separator, and run whisper on it. Enjoy it!\n",
        "\n",
        "(For sake of performance , I also tried attaching the audio segments into a single audio file with a silent -or beep- spacer as a separator, and run whisper on it see it on [colab](https://colab.research.google.com/drive/1HuvcY4tkTHPDzcwyVH77LCh_m8tP-Qet?usp=sharing). It [works](https://majdoddin.github.io/lexicap.html) on some audio, and fails on some (Dyson's Interview). The problem is, whisper does not reliably make a timestap on a spacer. See the discussions [#139](https://github.com/openai/whisper/discussions/139) and [#29](https://github.com/openai/whisper/discussions/29))\n",
        "\n",
        "The Markdown form used below is from [@ArthurFDLR](https://github.com/ArthurFDLR/whisper-youtube/).   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtljXaTXnowa"
      },
      "source": [
        "# Preparing the audio file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUA_4lnNgpSW"
      },
      "source": [
        "**Optional:** Mount Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3tqC8m8fyCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a58da67f-efe8-4f35-a197-99551dc45c47",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive_mount_path = Path(\"/content/drive\")\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "1zqnZsBacKph"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "#@markdown Enter the URL of the YouTube video, or the path to the video/audio file you want to transcribe, give the output path, etc. and run the cell. HTML file embeds the video for YouTube, and audio for media files.\n",
        "\n",
        "Source = 'Recording' #@param ['Youtube', 'File', 'Recording']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video**\n",
        "video_url = \"https://youtu.be/hpZFJctBUHQ\" #@param {type:\"string\"}\n",
        "#store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video or audio path (mp4, wav, mp3)**\n",
        "video_path = \"/content/5Second.mp3\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "output_path = \"/content/transcript/\" #@param {type:\"string\"}\n",
        "output_path = str(Path(output_path))\n",
        "#@markdown ---\n",
        "#@markdown #### **Title for transcription of media file**\n",
        "audio_title = \"Sample Order Taking\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Copy a token from your [Hugging Face tokens page](https://huggingface.co/settings/tokens) and paste it below.\n",
        "access_token = \"hf_\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvDON2GxZpIb"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vgK82ahXNje",
        "outputId": "e76d8acc-852b-4d2a-a661-00b50687ab5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transcript\n"
          ]
        }
      ],
      "source": [
        "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "%cd {output_path}\n",
        "video_title = \"\"\n",
        "video_id = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepating Audio File Length"
      ],
      "metadata": {
        "id": "8yo4_nR8mv9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# audioRange is used to get sections of the audio and convert the intervals we desire into ones the code can read\n",
        "def audioRange(length, time):\n",
        "  if time == \"sec\":\n",
        "    return length * 960\n",
        "  elif time == \"min\":\n",
        "    return (length * (60)) * 960\n",
        "  elif time == \"hour\":\n",
        "    return (length * (60 * 60)) * 960\n",
        "  else:\n",
        "    print(\"Invalid Time Interval\")\n",
        "    return -1"
      ],
      "metadata": {
        "id": "rkDXtt4tm3tO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def audioChange(audio, start, end):\n",
        "  duration = len(audio)\n",
        "  print(len(audio))\n",
        "  # Set a time for when the audio should begin. Currently, it's formatted by minutes and it starts at the beginning (0)\n",
        "  audioStart = start\n",
        "  audioEnd = end\n",
        "\n",
        "  if audioStart < 0:\n",
        "      print(\"STARTING TIME TOO EARLY: BEGINNING AT AUDIO START\")\n",
        "      audioStart = 0\n",
        "\n",
        "  if audioStart > audioEnd:\n",
        "      print(\"INCORRECT INTERVALS: READING FULL AUDIO\")\n",
        "      audioStart = 0\n",
        "      audioEnd = duration\n",
        "\n",
        "  if audioEnd > duration:\n",
        "      print(\"ENDING TIME TOO LATE: ENDING AT AUDIO END\")\n",
        "      audioEnd = duration\n",
        "\n",
        "\n",
        "  audio = audio[audioStart:audioEnd]\n",
        "  audio.export('newInput.wav', format='wav')\n",
        "  print(len(audio))"
      ],
      "metadata": {
        "id": "24Q3XAR1nV8x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-a6pLioVHjl"
      },
      "source": [
        "## From YouTube"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct4adIANpQvX"
      },
      "source": [
        " Installing [`yt-dlp`](https://github.com/yt-dlp/yt-dlp) and downloading the [video](https://youtu.be/NSp2fEQ6wyA) from youtube."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL7fC4aZdpyH"
      },
      "outputs": [],
      "source": [
        "if Source == \"Youtube\":\n",
        "  !pip install -U yt-dlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI5sr2GI4gXb"
      },
      "source": [
        "Custom build of `ffmpeg` as [recommended](https://github.com/yt-dlp/yt-dlp#strongly-recommended) by `yt-dlp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsedMPN-daEX"
      },
      "outputs": [],
      "source": [
        "if Source == \"Youtube\":\n",
        "  !wget -O - -q  https://github.com/yt-dlp/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz | xz -qdc| tar -x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNcD7tx9UmyK"
      },
      "outputs": [],
      "source": [
        "#Getting video info\n",
        "if Source == \"Youtube\":\n",
        "  from yt_dlp import YoutubeDL\n",
        "  with YoutubeDL() as ydl:\n",
        "    info_dict = ydl.extract_info(video_url, download=False)\n",
        "    video_title = info_dict.get('title', None)\n",
        "    video_id = info_dict.get('id', None)\n",
        "    print(\"Title: \" + video_title) # <= Here, you got the video title\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_UyK49aPNiD"
      },
      "source": [
        "Downloading the audio from YouTube."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VHzpv72dkvV"
      },
      "outputs": [],
      "source": [
        "if Source == \"Youtube\":\n",
        "  !yt-dlp -xv --ffmpeg-location ffmpeg-master-latest-linux64-gpl/bin --audio-format wav  -o \"{str(output_path) + '/'}input.wav\" -- {video_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJRyeo5RVYb8"
      },
      "source": [
        "## From File (Google Drive or Local)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D0gjXXtbk3tH",
        "outputId": "2cb8fc89-2933-40f9-a381-72a3d96cadfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "VjORT6CkVoTF",
        "outputId": "2e91125c-b010-4daa-9082-1f09509f82ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pydub'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6b4694d27a46>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mSource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'File'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ffmpeg -i {repr(video_path)} -vn -acodec pcm_s16le -ar 16000 -ac 1 -y input.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "if Source == 'File':\n",
        "    !ffmpeg -i {repr(video_path)} -vn -acodec pcm_s16le -ar 16000 -ac 1 -y input.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##From Recorded Audio"
      ],
      "metadata": {
        "id": "mQFNFp3v9vsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code inspired from Korakot Chaovavanich and others on GitHub https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be\n",
        "\n",
        "# Important imports:\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "# JavaScript to properly create a recording\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "# Used to record audio from the microphone, Time intervals have been added for seconds & minutes\n",
        "def record(time=\"sec\", interval=3) -> None:\n",
        "  display(Javascript(RECORD))\n",
        "  if time == \"sec\":\n",
        "    s = output.eval_js('record(%d)' % (interval*1000))\n",
        "  elif time == \"min\":\n",
        "    s = output.eval_js('record(%d)' % ((interval * 60) *1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  with open('record.wav','wb') as f:\n",
        "    f.write(b)\n",
        "  # Converts recording into useable format\n",
        "  !ffmpeg -i record.wav -vn -acodec pcm_s16le -ar 16000 -ac 1 -y input.wav\n",
        "\n",
        "if Source == 'Recording':\n",
        "    print(\"Do you want to record in minutes or seconds: \")\n",
        "    time = input()\n",
        "    print(\"How long do you want to record (Audio will be recored after): \")\n",
        "    interval = int(input())\n",
        "    print(\"Recording...\")\n",
        "    record(time, interval)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "Sge8VoUk95jL",
        "outputId": "39671cb7-e995-4094-c01a-00af6a24a119"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you want to record in minutes or seconds: \n",
            "sec\n",
            "How long do you want to record: \n",
            "10\n",
            "Recording...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, matroska,webm, from 'record.wav':\n",
            "  Metadata:\n",
            "    encoder         : Chrome\n",
            "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
            "  Stream #0:0(eng): Audio: opus, 48000 Hz, mono, fltp (default)\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (opus (native) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'input.wav':\n",
            "  Metadata:\n",
            "    ISFT            : Lavf58.76.100\n",
            "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 pcm_s16le\n",
            "size=       2kB time=00:00:00.00 bitrate=N/A speed=N/A    \rsize=     293kB time=00:00:09.35 bitrate= 256.1kbits/s speed= 345x    \n",
            "video:0kB audio:292kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.026042%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trimming the Audio"
      ],
      "metadata": {
        "id": "BdowwaRN9d5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "audio = AudioSegment.from_wav(\"input.wav\")\n",
        "\n",
        "# Set a time for when the audio should begin. Currently, it's formatted by minutes and it starts at the beginning (0)\n",
        "audioStart = audioRange(0, \"min\")\n",
        "audioEnd = audioRange(30, \"sec\")\n",
        "audioChange(audio, audioStart, audioEnd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlbdIVWf9eb8",
        "outputId": "347db216-e7ea-4e6e-b269-c3c8da2893d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9360\n",
            "4800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u1vbqd_VzNp"
      },
      "source": [
        "## Prepending a spacer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7qMLTISFE6M"
      },
      "source": [
        "`pyannote.audio` seems to miss the first 0.5 seconds of the audio, and, therefore, we prepend a spcacer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaRDsBV1CWi8",
        "outputId": "01427328-39c8-483d-ee67-06a6a64ad283"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='input_prep.wav'>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "spacermilli = 2000\n",
        "spacer = AudioSegment.silent(duration=spacermilli)\n",
        "\n",
        "\n",
        "audio = AudioSegment.from_wav(\"newInput.wav\")\n",
        "\n",
        "audio = spacer.append(audio, crossfade=0)\n",
        "\n",
        "audio.export('input_prep.wav', format='wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb5eEOKUooju"
      },
      "source": [
        "# Pyannote's Diarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxNf1l8Ye_U9"
      },
      "source": [
        "[`pyannote.audio`](https://github.com/pyannote/pyannote-audio) is an open-source toolkit written in Python for **speaker diarization**.\n",
        "\n",
        "Based on [`PyTorch`](https://pytorch.org) machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines.\n",
        "\n",
        "`pyannote.audio` also comes with pretrained [models](https://huggingface.co/models?other=pyannote-audio-model) and [pipelines](https://huggingface.co/models?other=pyannote-audio-pipeline) covering a wide range of domains for voice activity detection, speaker segmentation, overlapped speech detection, speaker embedding reaching state-of-the-art performance for most of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Ak_OQwqd-3"
      },
      "source": [
        "Installing `pyannote.audio`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFgcJ8f6dNUR",
        "outputId": "6a633e8c-4ee9-41f5-8da1-2305ef67d809",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: light-the-torch in /usr/local/lib/python3.10/dist-packages (0.7.5)\n",
            "Requirement already satisfied: pip<23.3,>=22.3 in /usr/local/lib/python3.10/dist-packages (from light-the-torch) (23.2.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install light-the-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhBEKgQGdPJc",
        "outputId": "27a78605-bb28-4b6a-fd02-ba32db847983",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!ltt install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vJGyKTQJqdzq",
        "outputId": "c1d43dc1-5a39-4882-e36b-41c53f43adf3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/hmmlearn/hmmlearn.git\n",
            "  Cloning https://github.com/hmmlearn/hmmlearn.git to /tmp/pip-req-build-vbn16nyx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hmmlearn/hmmlearn.git /tmp/pip-req-build-vbn16nyx\n",
            "  Resolved https://github.com/hmmlearn/hmmlearn.git to commit 33b1a916a11efb5c569abfc9705b7ae030e81c31\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn==0.3.2.post4+g33b1a91) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn==0.3.2.post4+g33b1a91) (1.3.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn==0.3.2.post4+g33b1a91) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn==0.3.2.post4+g33b1a91) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn==0.3.2.post4+g33b1a91) (3.5.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/pyannote/pyannote-audio.git@develop\n",
            "  Cloning https://github.com/pyannote/pyannote-audio.git (to revision develop) to /tmp/pip-req-build-mu1i_71s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyannote/pyannote-audio.git /tmp/pip-req-build-mu1i_71s\n",
            "  Resolved https://github.com/pyannote/pyannote-audio.git to commit 286ea1a4e34e2dd7d7926f590e402dac1e17494b\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (0.4.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (0.8.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (0.23.5)\n",
            "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (2.4.0)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (2.3.0)\n",
            "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (5.0.0)\n",
            "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (5.1.0)\n",
            "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (3.2.1)\n",
            "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (3.0.1)\n",
            "Requirement already satisfied: pytorch_metric_learning>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (2.6.0)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (13.7.1)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (3.0.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (0.12.1)\n",
            "Requirement already satisfied: speechbrain>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (1.0.0)\n",
            "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (2.6.2.2)\n",
            "Collecting torch>=2.0.0 (from pyannote.audio==3.3.1)\n",
            "  Obtaining dependency information for torch>=2.0.0 from https://files.pythonhosted.org/packages/9a/bd/4161ae28fb1c388a8ee30ca3aa72cf11ac3016ce62bc9e82c71ce193c410/torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
            "  Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torch_audiomentations>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (0.11.1)\n",
            "Collecting torchaudio>=2.2.0 (from pyannote.audio==3.3.1)\n",
            "  Obtaining dependency information for torchaudio>=2.2.0 from https://files.pythonhosted.org/packages/2d/4a/416af600d0d47343b65dd182fb7b918a640e80385b609d94fb89a3fc527c/torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
            "  Using cached torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.3.1) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio==3.3.1) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio==3.3.1) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (4.66.5)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio==3.3.1) (0.11.6)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio==3.3.1) (2.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio==3.3.1) (4.9.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.3.1) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.3.1) (1.13.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio==3.3.1) (2.1.4)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio==3.3.1) (0.12.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.3.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.3.1) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.3.1) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.3.1) (3.7.1)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.13.1)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (3.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.3.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.3.1) (2.16.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio==3.3.1) (1.16.0)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio==3.3.1) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio==3.3.1) (1.4.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio==3.3.1) (0.1.99)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.6->pyannote.audio==3.3.1) (3.20.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->pyannote.audio==3.3.1)\n",
            "  Obtaining dependency information for nvidia-cudnn-cu12==9.1.0.70 from https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\n",
            "Collecting triton==3.0.0 (from torch>=2.0.0->pyannote.audio==3.3.1)\n",
            "  Obtaining dependency information for triton==3.0.0 from https://files.pythonhosted.org/packages/45/27/14cc3101409b9b4b9241d2ba7deaa93535a217a211c86c4cc7151fb12181/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
            "  Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pyannote.audio==3.3.1) (12.6.20)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (0.2.7)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (0.10.2.post1)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (1.2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio==3.3.1) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (3.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (3.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (0.4.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio==3.3.1) (71.0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio==3.3.1) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (2.8.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (6.8.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (2.0.31)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio==3.3.1) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio==3.3.1) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.3.0)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.10/dist-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio==3.3.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio==3.3.1) (1.5.4)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio==3.3.1) (0.18.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio==3.3.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.13.0->pyannote.audio==3.3.1) (4.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (1.3.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio==3.3.1) (4.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.16.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio==3.3.1) (0.2.8)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (3.0.3)\n",
            "Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "Using cached torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, nvidia-cudnn-cu12, torch, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu117\n",
            "    Uninstalling torch-1.13.1+cu117:\n",
            "      Successfully uninstalled torch-1.13.1+cu117\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu117\n",
            "    Uninstalling torchaudio-0.13.1+cu117:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu117\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu117 requires torch==1.13.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 torch-2.4.0 torchaudio-2.4.0 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              },
              "id": "fd2329da80fb4b2080e2833706df4564"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install  git+https://github.com/hmmlearn/hmmlearn.git\n",
        "!pip install  git+https://github.com/pyannote/pyannote-audio.git@develop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7TPgEVW8XeH"
      },
      "source": [
        "**Important:** To load the pyannote speaker diarization pipeline,\n",
        "\n",
        "* accept the user conditions on both [hf.co/pyannote/speaker-diarization](https://hf.co/pyannote/speaker-diarization) and [hf.co/pyannote/segmentation](https://huggingface.co/pyannote/segmentation).\n",
        "* paste your access_token or login using `notebook_login` below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5u7VMb-YnqB"
      },
      "outputs": [],
      "source": [
        "if not(access_token):\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OiEq9nhzCKLM",
        "outputId": "141efe38-b8fb-4dd3-f806-d6e3926815ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch\n",
            "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/9a/bd/4161ae28fb1c388a8ee30ca3aa72cf11ac3016ce62bc9e82c71ce193c410/torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
            "  Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/42/c2/24b4416c53445098221557e18de0de59539cbe56b580b13a4f079746f3eb/torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
            "  Using cached torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torchaudio\n",
            "  Obtaining dependency information for torchaudio from https://files.pythonhosted.org/packages/2d/4a/416af600d0d47343b65dd182fb7b918a640e80385b609d94fb89a3fc527c/torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
            "  Using cached torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "Using cached torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "Using cached torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.4.0 torchaudio-2.4.0 torchvision-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKG14DGYbwku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4bb78ed-9833-4419-f0fd-895b81cd6aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.4.0+cu121. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        }
      ],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization', use_auth_token= (access_token) or True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjsUVdR2jH0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3efe80-44a1-49ac-f24a-bf9e63695bff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x78ecc1b0cc40>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipeline.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImKMcCr5W5Nw"
      },
      "source": [
        "Running pyannote.audio to generate the diarizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA4xiEefft9Z"
      },
      "outputs": [],
      "source": [
        "DEMO_FILE = {'uri': 'blabla', 'audio': 'input_prep.wav'}\n",
        "dz = pipeline(DEMO_FILE)\n",
        "\n",
        "with open(\"diarization.txt\", \"w\") as text_file:\n",
        "    text_file.write(str(dz))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHIY2MB3Vz3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd9aedb-2e9b-4c21-ef63-bfe8dd936530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<Segment(14.0372, 22.8797)>, 'A', 'SPEAKER_00')\n",
            "(<Segment(24.5166, 35.5697)>, 'B', 'SPEAKER_00')\n",
            "(<Segment(36.481, 56.9672)>, 'C', 'SPEAKER_00')\n",
            "(<Segment(58.0135, 59.971)>, 'D', 'SPEAKER_00')\n"
          ]
        }
      ],
      "source": [
        "print(*list(dz.itertracks(yield_label = True))[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp36eMedRkR0"
      },
      "source": [
        "# Preparing audio files according to the diarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPGOaVpOH7pZ"
      },
      "outputs": [],
      "source": [
        "def millisec(timeStr):\n",
        "  spl = timeStr.split(\":\")\n",
        "  s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n",
        "  return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Co3BIIH6aW4"
      },
      "source": [
        "Grouping the diarization segments according to the speaker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umQdzNFzcP2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94507f4-aba0-494a-c336-0071906b03a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[ 00:00:14.037 -->  00:00:22.879] A SPEAKER_00', '[ 00:00:24.516 -->  00:00:35.569] B SPEAKER_00', '[ 00:00:36.480 -->  00:00:56.967] C SPEAKER_00', '[ 00:00:58.013 -->  00:00:59.970] D SPEAKER_00']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "dzs = open('diarization.txt').read().splitlines()\n",
        "\n",
        "groups = []\n",
        "g = []\n",
        "lastend = 0\n",
        "\n",
        "for d in dzs:\n",
        "  if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
        "    groups.append(g)\n",
        "    g = []\n",
        "\n",
        "  g.append(d)\n",
        "\n",
        "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
        "  end = millisec(end)\n",
        "  if (lastend > end):       #segment engulfed by a previous segment\n",
        "    groups.append(g)\n",
        "    g = []\n",
        "  else:\n",
        "    lastend = end\n",
        "if g:\n",
        "  groups.append(g)\n",
        "print(*groups, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOuf8CuRQeZo"
      },
      "source": [
        "Save the audio part corresponding to each diarization group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRQPUW4Mzvfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f919b5-408c-44f0-8cc8-05fc613c1261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "group 0: 14037--59970\n"
          ]
        }
      ],
      "source": [
        "audio = AudioSegment.from_wav(\"input_prep.wav\")\n",
        "gidx = -1\n",
        "for g in groups:\n",
        "  start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
        "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
        "  start = millisec(start) #- spacermilli\n",
        "  end = millisec(end)  #- spacermilli\n",
        "  gidx += 1\n",
        "  audio[start:end].export(str(gidx) + '.wav', format='wav')\n",
        "  print(f\"group {gidx}: {start}--{end}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv2GYZCsLKBJ"
      },
      "source": [
        "Freeing up some memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cONumKWUjfus"
      },
      "outputs": [],
      "source": [
        "del   DEMO_FILE, pipeline, spacer,  audio, dz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmxtB0k4n8lY"
      },
      "source": [
        "# Whisper's Transcriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPVuqWaakkH"
      },
      "source": [
        "Installing Open AI whisper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUd7I__FUZVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "e75e9079-c09f-4227-9c74-9bef9be1ced7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-zk0xskw_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-zk0xskw_\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.3.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.7.0)\n",
            "Collecting triton<3,>=2.0.0 (from openai-whisper==20231117)\n",
            "  Obtaining dependency information for triton<3,>=2.0.0 from https://files.pythonhosted.org/packages/d7/69/8a9fde07d2d27a90e16488cdfe9878e985a247b2496a4b5b1a2126042528/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from openai-whisper==20231117)\n",
            "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/cb/e2/1bd899d3eb60c6495cf5d0d2885edacac08bde7a1407eadeb2ab36eca3c7/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
            "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117) (12.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802822 sha256=45508a6ce6d15370d8c405f740a91a0c9467eb1081da97dd5dda959f760ce173\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cyzln80z/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, nvidia-cudnn-cu12, torch, openai-whisper\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0\n",
            "    Uninstalling torch-2.4.0:\n",
            "      Successfully uninstalled torch-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.0 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 openai-whisper-20231117 torch-2.3.1 triton-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "triton"
                ]
              },
              "id": "03c4d2457f48476dbcaa3f1175cb0a74"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBO8IpdiRQ0X"
      },
      "source": [
        "Run whisper on all audio files. Whisper generates the transcription and writes it to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHKf0tFVmGyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b02bec5-9806-4723-b3cd-d9d8c3cee6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:53<00:00, 57.7MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = whisper.load_model('large', device = device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall triton -y\n",
        "!pip install triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "collapsed": true,
        "id": "0wtcNtYIbchZ",
        "outputId": "9df0a8e2-e87f-4e5a-d084-b68acda37cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: triton 2.3.1\n",
            "Uninstalling triton-2.3.1:\n",
            "  Successfully uninstalled triton-2.3.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting triton\n",
            "  Obtaining dependency information for triton from https://files.pythonhosted.org/packages/45/27/14cc3101409b9b4b9241d2ba7deaa93535a217a211c86c4cc7151fb12181/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
            "  Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.15.4)\n",
            "Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai-whisper 20231117 requires triton<3,>=2.0.0; platform_machine == \"x86_64\" and sys_platform == \"linux\" or sys_platform == \"linux2\", but you have triton 3.0.0 which is incompatible.\n",
            "torch 2.3.1 requires triton==2.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\", but you have triton 3.0.0 which is incompatible.\n",
            "torchaudio 2.4.0 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "db2ed25961f548598351b650ff791873"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odstu62EnMLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8c4684-2b82-45c2-d5a7-7ec95fffc5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " So, there's a small chance in this short amount of time that you've had a little moment to notice something about me.\n",
            " That was about five seconds. Five seconds is all it takes to have a first impression. First impressions.\n",
            " are the point of no return. You've started to judge the other person. A nicer way to say this is you've started to form an opinion\n",
            " or conclusion about that person. Your brain saw me and immediately tried to put me into categories, from most obvious to least.\n",
            " small, female, incredible fashion sense, and so on and so forth.\n",
            "\n",
            " So, there's a small chance in this short amount of time that you've had a little moment to notice something about me. That was about five seconds. Five seconds is all it takes to have a first impression. First impressions are the point of no return. You've started to judge the other person. A nicer way to say this is you've started to form an opinion or conclusion about that person. Your brain saw me and immediately tried to put me into categories from most obvious to least. Small, female, incredible fashion sense, and so on and so forth.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import whisper\n",
        "import math\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in range(len(groups)):\n",
        "  audiof = str(i) + '.wav'\n",
        "\n",
        "  \"\"\"\n",
        "   If the length of groups equals 1, this means there is only one speaker present and the result will transcribe in the form of one output.\n",
        "   This can be an issue especially for larger files as even with the timestamps given, the user can't know whether the info given is\n",
        "   accurate or not until the full audio is read and transcribed.\n",
        "\n",
        "   With the section duration, the user can edit how sectioned they want the audio in the form of seconds. This can help if the desire\n",
        "   is to separate the audio by minutes, hours, etc.\n",
        "\n",
        "   These intervals can help users see and figure out which interval allows for the best transcription/data collection.\n",
        "\n",
        "   Duration can also assist the users in choosing which interval of time they would prefer to use. For example, if the user wants\n",
        "   only half of the audio, they can multiply the duration by 0.5 to acheive this.\n",
        "\n",
        "   Additionally, for groups of multiple speakers, the audio is also printed to show what each speaker is saying.\n",
        "  \"\"\"\n",
        "  audio = AudioSegment.from_wav(audiof)\n",
        "\n",
        "\n",
        "  # Calculate the length of the audio file and find the output of each section\n",
        "  # Keep in mind, duration is not in a valid time interval. In order to get it in seconds, divide by 805\n",
        "  duration = len(audio)\n",
        "\n",
        "  # Find the section duration and have it so each 10 seconds is printed.\n",
        "  sectionDuration = audioRange(10, \"sec\")\n",
        "\n",
        "  if(len(groups) == 1):\n",
        "\n",
        "    for i in range(audioStart, audioEnd, sectionDuration):\n",
        "\n",
        "      # Create a new audio segment for the specific 10 second section\n",
        "      part = audio[i:i+sectionDuration]\n",
        "\n",
        "      # Export the audio segment to a usable file\n",
        "      part.export('part1.wav', format='wav')\n",
        "\n",
        "      # Transcribe the section individually to determine optimal\n",
        "      tempResult = model.transcribe(audio=\"part1.wav\", language='en', word_timestamps=True)\n",
        "\n",
        "      # Print the results to establish the model is working as intended as well as determine\n",
        "      print(tempResult[\"text\"])\n",
        "      results.append(tempResult)\n",
        "\n",
        "  result = model.transcribe(audio=audiof, language='en', word_timestamps=True)#, initial_prompt=result.get('text', \"\"))\n",
        "  print(result[\"text\"])\n",
        "  with open(str(i)+'.json', \"w\") as outfile:\n",
        "    json.dump(result, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mC-4Wo3X20z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3743238e-d84d-43dc-e77d-c1db60303b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_UyWQMXpB3N"
      },
      "source": [
        "# Generating the HTML and/or txt file from the Transcriptions and the Diarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2qTkKD_30FG"
      },
      "source": [
        "Change or add to the speaker names and collors bellow as you wish `(speaker, textbox color, speaker color)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7EP6fO73wTY"
      },
      "outputs": [],
      "source": [
        "speakers = {'SPEAKER_00':('TED Talker', '#e1ffc7', 'darkgreen'), 'SPEAKER_01':('SPEAKER_01', 'white', 'darkorange'), 'SPEAKER_05':('Carvel Jones', 'white', 'darkorange')}\n",
        "def_boxclr = 'white'\n",
        "def_spkrclr = 'orange'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KndDYy_xMpMq"
      },
      "source": [
        "In the generated HTML,  the transcriptions for each diarization group are written in a box, with the speaker name on the top. By clicking a transcription, the embedded video jumps to the right time ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKdx9Hwg630K"
      },
      "outputs": [],
      "source": [
        "if Source == 'Youtube':\n",
        "    preS = '<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n\\t<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\\n\\t<title>' + \\\n",
        "video_title+ \\\n",
        "'</title>\\n\\t<style>\\n\\t\\tbody {\\n\\t\\t\\tfont-family: sans-serif;\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\tcolor: #111;\\n\\t\\t\\tpadding: 0 0 1em 0;\\n\\t\\t\\tbackground-color: #efe7dd;\\n\\t\\t}\\n\\n\\t\\ttable {\\n\\t\\t\\tborder-spacing: 10px;\\n\\t\\t}\\n\\n\\t\\tth {\\n\\t\\t\\ttext-align: left;\\n\\t\\t}\\n\\n\\t\\t.lt {\\n\\t\\t\\tcolor: inherit;\\n\\t\\t\\ttext-decoration: inherit;\\n\\t\\t}\\n\\n\\t\\t.l {\\n\\t\\t\\tcolor: #050;\\n\\t\\t}\\n\\n\\t\\t.s {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.c {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.e {\\n\\t\\t\\t/*background-color: white; Changing background color */\\n\\t\\t\\tborder-radius: 10px;\\n\\t\\t\\t/* Making border radius */\\n\\t\\t\\twidth: 50%;\\n\\t\\t\\t/* Making auto-sizable width */\\n\\t\\t\\tpadding: 0 0 0 0;\\n\\t\\t\\t/* Making space around letters */\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\t/* Changing font size */\\n\\t\\t\\tmargin-bottom: 0;\\n\\t\\t}\\n\\n\\t\\t.t {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t#player-div {\\n\\t\\t\\tposition: sticky;\\n\\t\\t\\ttop: 20px;\\n\\t\\t\\tfloat: right;\\n\\t\\t\\twidth: 40%\\n\\t\\t}\\n\\n\\t\\t#player {\\n\\t\\t\\taspect-ratio: 16 / 9;\\n\\t\\t\\twidth: 100%;\\n\\t\\t\\theight: auto;\\n\\n\\t\\t}\\n\\n\\t\\ta {\\n\\t\\t\\tdisplay: inline;\\n\\t\\t}\\n\\t</style>\\n\\t<script>\\n\\t\\tvar tag = document.createElement(\\'script\\');\\n\\t\\ttag.src = \"https://www.youtube.com/iframe_api\";\\n\\t\\tvar firstScriptTag = document.getElementsByTagName(\\'script\\')[0];\\n\\t\\tfirstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\\n\\t\\tvar player;\\n\\t\\tfunction onYouTubeIframeAPIReady() {\\n\\t\\t\\tplayer = new YT.Player(\\'player\\', {\\n\\t\\t\\t\\t//height: \\'210\\',\\n\\t\\t\\t\\t//width: \\'340\\',\\n\\t\\t\\t\\tvideoId: \\''+ \\\n",
        "video_id + \\\n",
        "'\\',\\n\\t\\t\\t});\\n\\n\\n\\n\\t\\t\\t// This is the source \"window\" that will emit the events.\\n\\t\\t\\tvar iframeWindow = player.getIframe().contentWindow;\\n\\t\\t\\tvar lastword = null;\\n\\n\\t\\t\\t// So we can compare against new updates.\\n\\t\\t\\tvar lastTimeUpdate = \"-1\";\\n\\n\\t\\t\\t// Listen to events triggered by postMessage,\\n\\t\\t\\t// this is how different windows in a browser\\n\\t\\t\\t// (such as a popup or iFrame) can communicate.\\n\\t\\t\\t// See: https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage\\n\\t\\t\\twindow.addEventListener(\"message\", function (event) {\\n\\t\\t\\t\\t// Check that the event was sent from the YouTube IFrame.\\n\\t\\t\\t\\tif (event.source === iframeWindow) {\\n\\t\\t\\t\\t\\tvar data = JSON.parse(event.data);\\n\\n\\t\\t\\t\\t\\t// The \"infoDelivery\" event is used by YT to transmit any\\n\\t\\t\\t\\t\\t// kind of information change in the player,\\n\\t\\t\\t\\t\\t// such as the current time or a playback quality change.\\n\\t\\t\\t\\t\\tif (\\n\\t\\t\\t\\t\\t\\tdata.event === \"infoDelivery\" &&\\n\\t\\t\\t\\t\\t\\tdata.info &&\\n\\t\\t\\t\\t\\t\\tdata.info.currentTime\\n\\t\\t\\t\\t\\t) {\\n\\t\\t\\t\\t\\t\\t// currentTime is emitted very frequently (milliseconds),\\n\\t\\t\\t\\t\\t\\t// but we only care about whole second changes.\\n\\t\\t\\t\\t\\t\\tvar ts = (data.info.currentTime).toFixed(1).toString();\\n\\t\\t\\t\\t\\t\\tts = (Math.round((data.info.currentTime) * 5) / 5).toFixed(1);\\n\\t\\t\\t\\t\\t\\tts = ts.toString();\\n\\t\\t\\t\\t\\t\\tconsole.log(ts)\\n\\t\\t\\t\\t\\t\\tif (ts !== lastTimeUpdate) {\\n\\t\\t\\t\\t\\t\\t\\tlastTimeUpdate = ts;\\n\\n\\t\\t\\t\\t\\t\\t\\t// It\\'s now up to you to format the time.\\n\\t\\t\\t\\t\\t\\t\\t//document.getElementById(\"time2\").innerHTML = time;\\n\\t\\t\\t\\t\\t\\t\\tword = document.getElementById(ts)\\n\\t\\t\\t\\t\\t\\t\\tif (word) {\\n\\t\\t\\t\\t\\t\\t\\t\\tif (lastword) {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlastword.style.fontWeight = \\'normal\\';\\n\\t\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\t\\tlastword = word;\\n\\t\\t\\t\\t\\t\\t\\t\\t//word.style.textDecoration = \\'underline\\';\\n\\t\\t\\t\\t\\t\\t\\t\\tword.style.fontWeight = \\'bold\\';\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tlet toggle = document.getElementById(\"autoscroll\");\\n\\t\\t\\t\\t\\t\\t\\t\\tif (toggle.checked) {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlet position = word.offsetTop - 20;\\n\\t\\t\\t\\t\\t\\t\\t\\t\\twindow.scrollTo({\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttop: position,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tbehavior: \\'smooth\\'\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t})\\n\\t\\t}\\n\\t\\tfunction jumptoTime(timepoint, id) {\\n\\t\\t\\tevent.preventDefault();\\n\\t\\t\\thistory.pushState(null, null, \"#\" + id);\\n\\t\\t\\tplayer.seekTo(timepoint);\\n\\t\\t\\tplayer.playVideo();\\n\\t\\t}\\n\\t</script>\\n</head>\\n\\n<body>\\n\\t<h2>'  + \\\n",
        "video_title + \\\n",
        "'</h2>\\n\\t<i>Click on a part of the transcription, to jump to its video, and get an anchor to it in the address\\n\\t\\tbar<br><br></i>\\n\\t<div id=\"player-div\">\\n\\t\\t<div id=\"player\"></div>\\n\\t\\t<div><label for=\"autoscroll\">auto-scroll: </label>\\n\\t\\t\\t<input type=\"checkbox\" id=\"autoscroll\" checked>\\n\\t\\t</div>\\n\\t</div>\\n  '\n",
        "else:\n",
        "    preS = '\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"whtmlidth=device-width, initial-scale=1.0\">\\n\\t<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\\n\\t<title>' + \\\n",
        "    audio_title+ \\\n",
        "    '</title>\\n\\t<style>\\n\\t\\tbody {\\n\\t\\t\\tfont-family: sans-serif;\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\tcolor: #111;\\n\\t\\t\\tpadding: 0 0 1em 0;\\n\\t\\t\\tbackground-color: #efe7dd;\\n\\t\\t}\\n\\n\\t\\ttable {\\n\\t\\t\\tborder-spacing: 10px;\\n\\t\\t}\\n\\n\\t\\tth {\\n\\t\\t\\ttext-align: left;\\n\\t\\t}\\n\\n\\t\\t.lt {\\n\\t\\t\\tcolor: inherit;\\n\\t\\t\\ttext-decoration: inherit;\\n\\t\\t}\\n\\n\\t\\t.l {\\n\\t\\t\\tcolor: #050;\\n\\t\\t}\\n\\n\\t\\t.s {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.c {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.e {\\n\\t\\t\\t/*background-color: white; Changing background color */\\n\\t\\t\\tborder-radius: 10px;\\n\\t\\t\\t/* Making border radius */\\n\\t\\t\\twidth: 50%;\\n\\t\\t\\t/* Making auto-sizable width */\\n\\t\\t\\tpadding: 0 0 0 0;\\n\\t\\t\\t/* Making space around letters */\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\t/* Changing font size */\\n\\t\\t\\tmargin-bottom: 0;\\n\\t\\t}\\n\\n\\t\\t.t {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t#player-div {\\n\\t\\t\\tposition: sticky;\\n\\t\\t\\ttop: 20px;\\n\\t\\t\\tfloat: right;\\n\\t\\t\\twidth: 40%\\n\\t\\t}\\n\\n\\t\\t#player {\\n\\t\\t\\taspect-ratio: 16 / 9;\\n\\t\\t\\twidth: 100%;\\n\\t\\t\\theight: auto;\\n\\t\\t}\\n\\n\\t\\ta {\\n\\t\\t\\tdisplay: inline;\\n\\t\\t}\\n\\t</style>';\n",
        "    preS += '\\n\\t<script>\\n\\twindow.onload = function () {\\n\\t\\t\\tvar player = document.getElementById(\"audio_player\");\\n\\t\\t\\tvar player;\\n\\t\\t\\tvar lastword = null;\\n\\n\\t\\t\\t// So we can compare against new updates.\\n\\t\\t\\tvar lastTimeUpdate = \"-1\";\\n\\n\\t\\t\\tsetInterval(function () {\\n\\t\\t\\t\\t// currentTime is checked very frequently (1 millisecond),\\n\\t\\t\\t\\t// but we only care about whole second changes.\\n\\t\\t\\t\\tvar ts = (player.currentTime).toFixed(1).toString();\\n\\t\\t\\t\\tts = (Math.round((player.currentTime) * 5) / 5).toFixed(1);\\n\\t\\t\\t\\tts = ts.toString();\\n\\t\\t\\t\\tconsole.log(ts);\\n\\t\\t\\t\\tif (ts !== lastTimeUpdate) {\\n\\t\\t\\t\\t\\tlastTimeUpdate = ts;\\n\\n\\t\\t\\t\\t\\t// Its now up to you to format the time.\\n\\t\\t\\t\\t\\tword = document.getElementById(ts)\\n\\t\\t\\t\\t\\tif (word) {\\n\\t\\t\\t\\t\\t\\tif (lastword) {\\n\\t\\t\\t\\t\\t\\t\\tlastword.style.fontWeight = \"normal\";\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tlastword = word;\\n\\t\\t\\t\\t\\t\\t//word.style.textDecoration = \"underline\";\\n\\t\\t\\t\\t\\t\\tword.style.fontWeight = \"bold\";\\n\\n\\t\\t\\t\\t\\t\\tlet toggle = document.getElementById(\"autoscroll\");\\n\\t\\t\\t\\t\\t\\tif (toggle.checked) {\\n\\t\\t\\t\\t\\t\\t\\tlet position = word.offsetTop - 20;\\n\\t\\t\\t\\t\\t\\t\\twindow.scrollTo({\\n\\t\\t\\t\\t\\t\\t\\t\\ttop: position,\\n\\t\\t\\t\\t\\t\\t\\t\\tbehavior: \"smooth\"\\n\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}, 0.1);\\n\\t\\t}\\n\\n\\t\\tfunction jumptoTime(timepoint, id) {\\n\\t\\t\\tvar player = document.getElementById(\"audio_player\");\\n\\t\\t\\thistory.pushState(null, null, \"#\" + id);\\n\\t\\t\\tplayer.pause();\\n\\t\\t\\tplayer.currentTime = timepoint;\\n\\t\\t\\tplayer.play();\\n\\t\\t}\\n\\t\\t</script>\\n\\t</head>';\n",
        "    preS += '\\n\\n<body>\\n\\t<h2>' + audio_title + '</h2>\\n\\t<i>Click on a part of the transcription, to jump to its portion of audio, and get an anchor to it in the address\\n\\t\\tbar<br><br></i>\\n\\t<div id=\"player-div\">\\n\\t\\t<div id=\"player\">\\n\\t\\t\\t<audio controls=\"controls\" id=\"audio_player\">\\n\\t\\t\\t\\t<source src=\"input.wav\" />\\n\\t\\t\\t</audio>\\n\\t\\t</div>\\n\\t\\t<div><label for=\"autoscroll\">auto-scroll: </label>\\n\\t\\t\\t<input type=\"checkbox\" id=\"autoscroll\" checked>\\n\\t\\t</div>\\n\\t</div>\\n';\n",
        "\n",
        "postS = '\\t</body>\\n</html>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqO6Nd6YfZYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dc0fd6-78fb-44e7-f789-3a219326f3ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions saved to capspeaker.txt:\n",
            "[00:00:012.04 --> 00:00:016.44] [TED Talker]  So, there's a small chance in this short amount of time\n",
            "[00:00:017.22 --> 00:00:020.50] [TED Talker]  that you've had a little moment to notice something about me.\n",
            "[00:00:022.50 --> 00:00:023.80] [TED Talker]  That was about five seconds.\n",
            "[00:00:024.54 --> 00:00:028.48] [TED Talker]  Five seconds is all it takes to have a first impression.\n",
            "[00:00:029.42 --> 00:00:033.10] [TED Talker]  First impressions are the point of no return.\n",
            "[00:00:034.04 --> 00:00:036.36] [TED Talker]  You've started to judge the other person.\n",
            "[00:00:037.60 --> 00:00:040.86] [TED Talker]  A nicer way to say this is you've started to form an opinion\n",
            "[00:00:040.86 --> 00:00:043.30] [TED Talker]  or conclusion about that person.\n",
            "[00:00:044.10 --> 00:00:047.68] [TED Talker]  Your brain saw me and immediately tried to put me into categories\n",
            "[00:00:048.30 --> 00:00:049.96] [TED Talker]  from most obvious to least.\n",
            "[00:00:050.64 --> 00:00:054.52] [TED Talker]  Small, female, incredible fashion sense,\n",
            "[00:00:055.36 --> 00:00:057.42] [TED Talker]  and so on and so forth.\n",
            "\n",
            "\n",
            "captions saved to capspeaker.html:\n",
            "\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "\n",
            "<head>\n",
            "\t<meta charset=\"UTF-8\">\n",
            "\t<meta name=\"viewport\" content=\"whtmlidth=device-width, initial-scale=1.0\">\n",
            "\t<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n",
            "\t<title>Sample Order Taking</title>\n",
            "\t<style>\n",
            "\t\tbody {\n",
            "\t\t\tfont-family: sans-serif;\n",
            "\t\t\tfont-size: 14px;\n",
            "\t\t\tcolor: #111;\n",
            "\t\t\tpadding: 0 0 1em 0;\n",
            "\t\t\tbackground-color: #efe7dd;\n",
            "\t\t}\n",
            "\n",
            "\t\ttable {\n",
            "\t\t\tborder-spacing: 10px;\n",
            "\t\t}\n",
            "\n",
            "\t\tth {\n",
            "\t\t\ttext-align: left;\n",
            "\t\t}\n",
            "\n",
            "\t\t.lt {\n",
            "\t\t\tcolor: inherit;\n",
            "\t\t\ttext-decoration: inherit;\n",
            "\t\t}\n",
            "\n",
            "\t\t.l {\n",
            "\t\t\tcolor: #050;\n",
            "\t\t}\n",
            "\n",
            "\t\t.s {\n",
            "\t\t\tdisplay: inline-block;\n",
            "\t\t}\n",
            "\n",
            "\t\t.c {\n",
            "\t\t\tdisplay: inline-block;\n",
            "\t\t}\n",
            "\n",
            "\t\t.e {\n",
            "\t\t\t/*background-color: white; Changing background color */\n",
            "\t\t\tborder-radius: 10px;\n",
            "\t\t\t/* Making border radius */\n",
            "\t\t\twidth: 50%;\n",
            "\t\t\t/* Making auto-sizable width */\n",
            "\t\t\tpadding: 0 0 0 0;\n",
            "\t\t\t/* Making space around letters */\n",
            "\t\t\tfont-size: 14px;\n",
            "\t\t\t/* Changing font size */\n",
            "\t\t\tmargin-bottom: 0;\n",
            "\t\t}\n",
            "\n",
            "\t\t.t {\n",
            "\t\t\tdisplay: inline-block;\n",
            "\t\t}\n",
            "\n",
            "\t\t#player-div {\n",
            "\t\t\tposition: sticky;\n",
            "\t\t\ttop: 20px;\n",
            "\t\t\tfloat: right;\n",
            "\t\t\twidth: 40%\n",
            "\t\t}\n",
            "\n",
            "\t\t#player {\n",
            "\t\t\taspect-ratio: 16 / 9;\n",
            "\t\t\twidth: 100%;\n",
            "\t\t\theight: auto;\n",
            "\t\t}\n",
            "\n",
            "\t\ta {\n",
            "\t\t\tdisplay: inline;\n",
            "\t\t}\n",
            "\t</style>\n",
            "\t<script>\n",
            "\twindow.onload = function () {\n",
            "\t\t\tvar player = document.getElementById(\"audio_player\");\n",
            "\t\t\tvar player;\n",
            "\t\t\tvar lastword = null;\n",
            "\n",
            "\t\t\t// So we can compare against new updates.\n",
            "\t\t\tvar lastTimeUpdate = \"-1\";\n",
            "\n",
            "\t\t\tsetInterval(function () {\n",
            "\t\t\t\t// currentTime is checked very frequently (1 millisecond),\n",
            "\t\t\t\t// but we only care about whole second changes.\n",
            "\t\t\t\tvar ts = (player.currentTime).toFixed(1).toString();\n",
            "\t\t\t\tts = (Math.round((player.currentTime) * 5) / 5).toFixed(1);\n",
            "\t\t\t\tts = ts.toString();\n",
            "\t\t\t\tconsole.log(ts);\n",
            "\t\t\t\tif (ts !== lastTimeUpdate) {\n",
            "\t\t\t\t\tlastTimeUpdate = ts;\n",
            "\n",
            "\t\t\t\t\t// Its now up to you to format the time.\n",
            "\t\t\t\t\tword = document.getElementById(ts)\n",
            "\t\t\t\t\tif (word) {\n",
            "\t\t\t\t\t\tif (lastword) {\n",
            "\t\t\t\t\t\t\tlastword.style.fontWeight = \"normal\";\n",
            "\t\t\t\t\t\t}\n",
            "\t\t\t\t\t\tlastword = word;\n",
            "\t\t\t\t\t\t//word.style.textDecoration = \"underline\";\n",
            "\t\t\t\t\t\tword.style.fontWeight = \"bold\";\n",
            "\n",
            "\t\t\t\t\t\tlet toggle = document.getElementById(\"autoscroll\");\n",
            "\t\t\t\t\t\tif (toggle.checked) {\n",
            "\t\t\t\t\t\t\tlet position = word.offsetTop - 20;\n",
            "\t\t\t\t\t\t\twindow.scrollTo({\n",
            "\t\t\t\t\t\t\t\ttop: position,\n",
            "\t\t\t\t\t\t\t\tbehavior: \"smooth\"\n",
            "\t\t\t\t\t\t\t});\n",
            "\t\t\t\t\t\t}\n",
            "\t\t\t\t\t}\n",
            "\t\t\t\t}\n",
            "\t\t\t}, 0.1);\n",
            "\t\t}\n",
            "\n",
            "\t\tfunction jumptoTime(timepoint, id) {\n",
            "\t\t\tvar player = document.getElementById(\"audio_player\");\n",
            "\t\t\thistory.pushState(null, null, \"#\" + id);\n",
            "\t\t\tplayer.pause();\n",
            "\t\t\tplayer.currentTime = timepoint;\n",
            "\t\t\tplayer.play();\n",
            "\t\t}\n",
            "\t\t</script>\n",
            "\t</head>\n",
            "\n",
            "<body>\n",
            "\t<h2>Sample Order Taking</h2>\n",
            "\t<i>Click on a part of the transcription, to jump to its portion of audio, and get an anchor to it in the address\n",
            "\t\tbar<br><br></i>\n",
            "\t<div id=\"player-div\">\n",
            "\t\t<div id=\"player\">\n",
            "\t\t\t<audio controls=\"controls\" id=\"audio_player\">\n",
            "\t\t\t\t<source src=\"input.wav\" />\n",
            "\t\t\t</audio>\n",
            "\t\t</div>\n",
            "\t\t<div><label for=\"autoscroll\">auto-scroll: </label>\n",
            "\t\t\t<input type=\"checkbox\" id=\"autoscroll\" checked>\n",
            "\t\t</div>\n",
            "\t</div>\n",
            "<div class=\"e\" style=\"background-color: #e1ffc7\">\n",
            "<p  style=\"margin:0;padding: 5px 10px 10px 10px;word-wrap:normal;white-space:normal;\">\n",
            "<span style=\"color:darkgreen;font-weight: bold;\">TED Talker</span><br>\n",
            "\t\t\t\t<a href=\"#00:00:012.04\" id=\"12.0\" class=\"lt\" onclick=\"jumptoTime(12, this.id)\"> So,</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:012.40\" id=\"12.4\" class=\"lt\" onclick=\"jumptoTime(12, this.id)\"> there's</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:013.18\" id=\"13.2\" class=\"lt\" onclick=\"jumptoTime(13, this.id)\"> a</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:013.30\" id=\"13.2\" class=\"lt\" onclick=\"jumptoTime(13, this.id)\"> small</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:013.72\" id=\"13.8\" class=\"lt\" onclick=\"jumptoTime(13, this.id)\"> chance</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:014.14\" id=\"14.2\" class=\"lt\" onclick=\"jumptoTime(14, this.id)\"> in</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:014.64\" id=\"14.6\" class=\"lt\" onclick=\"jumptoTime(14, this.id)\"> this</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:014.92\" id=\"15.0\" class=\"lt\" onclick=\"jumptoTime(14, this.id)\"> short</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:015.70\" id=\"15.6\" class=\"lt\" onclick=\"jumptoTime(15, this.id)\"> amount</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:015.94\" id=\"16.0\" class=\"lt\" onclick=\"jumptoTime(15, this.id)\"> of</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:016.12\" id=\"16.2\" class=\"lt\" onclick=\"jumptoTime(16, this.id)\"> time</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:017.00\" id=\"17.0\" class=\"lt\" onclick=\"jumptoTime(16, this.id)\"> that</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:017.32\" id=\"17.4\" class=\"lt\" onclick=\"jumptoTime(17, this.id)\"> you've</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:017.80\" id=\"17.8\" class=\"lt\" onclick=\"jumptoTime(17, this.id)\"> had</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:018.02\" id=\"18.0\" class=\"lt\" onclick=\"jumptoTime(18, this.id)\"> a</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:018.14\" id=\"18.2\" class=\"lt\" onclick=\"jumptoTime(18, this.id)\"> little</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:018.42\" id=\"18.4\" class=\"lt\" onclick=\"jumptoTime(18, this.id)\"> moment</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:018.84\" id=\"18.8\" class=\"lt\" onclick=\"jumptoTime(18, this.id)\"> to</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:019.22\" id=\"19.2\" class=\"lt\" onclick=\"jumptoTime(19, this.id)\"> notice</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:019.54\" id=\"19.6\" class=\"lt\" onclick=\"jumptoTime(19, this.id)\"> something</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:019.88\" id=\"19.8\" class=\"lt\" onclick=\"jumptoTime(19, this.id)\"> about</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:020.22\" id=\"20.2\" class=\"lt\" onclick=\"jumptoTime(20, this.id)\"> me.</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:022.22\" id=\"22.2\" class=\"lt\" onclick=\"jumptoTime(22, this.id)\"> That</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:022.54\" id=\"22.6\" class=\"lt\" onclick=\"jumptoTime(22, this.id)\"> was</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:022.76\" id=\"22.8\" class=\"lt\" onclick=\"jumptoTime(22, this.id)\"> about</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:022.98\" id=\"23.0\" class=\"lt\" onclick=\"jumptoTime(22, this.id)\"> five</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:023.32\" id=\"23.4\" class=\"lt\" onclick=\"jumptoTime(23, this.id)\"> seconds.</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:024.54\" id=\"24.6\" class=\"lt\" onclick=\"jumptoTime(24, this.id)\"> Five</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:025.18\" id=\"25.2\" class=\"lt\" onclick=\"jumptoTime(25, this.id)\"> seconds</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:025.62\" id=\"25.6\" class=\"lt\" onclick=\"jumptoTime(25, this.id)\"> is</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:026.50\" id=\"26.4\" class=\"lt\" onclick=\"jumptoTime(26, this.id)\"> all</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:026.78\" id=\"26.8\" class=\"lt\" onclick=\"jumptoTime(26, this.id)\"> it</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:026.98\" id=\"27.0\" class=\"lt\" onclick=\"jumptoTime(26, this.id)\"> takes</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:027.26\" id=\"27.2\" class=\"lt\" onclick=\"jumptoTime(27, this.id)\"> to</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:027.60\" id=\"27.6\" class=\"lt\" onclick=\"jumptoTime(27, this.id)\"> have</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:027.74\" id=\"27.8\" class=\"lt\" onclick=\"jumptoTime(27, this.id)\"> a</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:027.88\" id=\"27.8\" class=\"lt\" onclick=\"jumptoTime(27, this.id)\"> first</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:028.08\" id=\"28.0\" class=\"lt\" onclick=\"jumptoTime(28, this.id)\"> impression.</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:029.42\" id=\"29.4\" class=\"lt\" onclick=\"jumptoTime(29, this.id)\"> First</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:030.06\" id=\"30.0\" class=\"lt\" onclick=\"jumptoTime(30, this.id)\"> impressions</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:030.70\" id=\"30.6\" class=\"lt\" onclick=\"jumptoTime(30, this.id)\"> are</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:031.60\" id=\"31.6\" class=\"lt\" onclick=\"jumptoTime(31, this.id)\"> the</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:031.76\" id=\"31.8\" class=\"lt\" onclick=\"jumptoTime(31, this.id)\"> point</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:032.10\" id=\"32.0\" class=\"lt\" onclick=\"jumptoTime(32, this.id)\"> of</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:032.42\" id=\"32.4\" class=\"lt\" onclick=\"jumptoTime(32, this.id)\"> no</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:032.66\" id=\"32.6\" class=\"lt\" onclick=\"jumptoTime(32, this.id)\"> return.</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:034.04\" id=\"34.0\" class=\"lt\" onclick=\"jumptoTime(34, this.id)\"> You've</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:034.68\" id=\"34.6\" class=\"lt\" onclick=\"jumptoTime(34, this.id)\"> started</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:035.02\" id=\"35.0\" class=\"lt\" onclick=\"jumptoTime(35, this.id)\"> to</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:035.26\" id=\"35.2\" class=\"lt\" onclick=\"jumptoTime(35, this.id)\"> judge</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:035.52\" id=\"35.6\" class=\"lt\" onclick=\"jumptoTime(35, this.id)\"> the</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:035.76\" id=\"35.8\" class=\"lt\" onclick=\"jumptoTime(35, this.id)\"> other</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:035.94\" id=\"36.0\" class=\"lt\" onclick=\"jumptoTime(35, this.id)\"> person.</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:037.30\" id=\"37.2\" class=\"lt\" onclick=\"jumptoTime(37, this.id)\"> A</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:037.62\" id=\"37.6\" class=\"lt\" onclick=\"jumptoTime(37, this.id)\"> nicer</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:038.02\" id=\"38.0\" class=\"lt\" onclick=\"jumptoTime(38, this.id)\"> way</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:038.30\" id=\"38.2\" class=\"lt\" onclick=\"jumptoTime(38, this.id)\"> to</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:038.46\" id=\"38.4\" class=\"lt\" onclick=\"jumptoTime(38, this.id)\"> say</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:038.68\" id=\"38.6\" class=\"lt\" onclick=\"jumptoTime(38, this.id)\"> this</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:038.96\" id=\"39.0\" class=\"lt\" onclick=\"jumptoTime(38, this.id)\"> is</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:039.22\" id=\"39.2\" class=\"lt\" onclick=\"jumptoTime(39, this.id)\"> you've</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:039.60\" id=\"39.6\" class=\"lt\" onclick=\"jumptoTime(39, this.id)\"> started</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:039.88\" id=\"39.8\" class=\"lt\" onclick=\"jumptoTime(39, this.id)\"> to</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:040.06\" id=\"40.0\" class=\"lt\" onclick=\"jumptoTime(40, this.id)\"> form</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:040.30\" id=\"40.2\" class=\"lt\" onclick=\"jumptoTime(40, this.id)\"> an</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:040.46\" id=\"40.4\" class=\"lt\" onclick=\"jumptoTime(40, this.id)\"> opinion</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:040.86\" id=\"40.8\" class=\"lt\" onclick=\"jumptoTime(40, this.id)\"> or</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:041.48\" id=\"41.4\" class=\"lt\" onclick=\"jumptoTime(41, this.id)\"> conclusion</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:041.96\" id=\"42.0\" class=\"lt\" onclick=\"jumptoTime(41, this.id)\"> about</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:042.52\" id=\"42.6\" class=\"lt\" onclick=\"jumptoTime(42, this.id)\"> that</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:042.84\" id=\"42.8\" class=\"lt\" onclick=\"jumptoTime(42, this.id)\"> person.</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:043.92\" id=\"44.0\" class=\"lt\" onclick=\"jumptoTime(43, this.id)\"> Your</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:044.28\" id=\"44.2\" class=\"lt\" onclick=\"jumptoTime(44, this.id)\"> brain</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:044.64\" id=\"44.6\" class=\"lt\" onclick=\"jumptoTime(44, this.id)\"> saw</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:045.12\" id=\"45.2\" class=\"lt\" onclick=\"jumptoTime(45, this.id)\"> me</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:045.42\" id=\"45.4\" class=\"lt\" onclick=\"jumptoTime(45, this.id)\"> and</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:045.84\" id=\"45.8\" class=\"lt\" onclick=\"jumptoTime(45, this.id)\"> immediately</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:046.20\" id=\"46.2\" class=\"lt\" onclick=\"jumptoTime(46, this.id)\"> tried</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:046.58\" id=\"46.6\" class=\"lt\" onclick=\"jumptoTime(46, this.id)\"> to</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:046.78\" id=\"46.8\" class=\"lt\" onclick=\"jumptoTime(46, this.id)\"> put</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:046.94\" id=\"47.0\" class=\"lt\" onclick=\"jumptoTime(46, this.id)\"> me</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:047.06\" id=\"47.0\" class=\"lt\" onclick=\"jumptoTime(47, this.id)\"> into</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:047.22\" id=\"47.2\" class=\"lt\" onclick=\"jumptoTime(47, this.id)\"> categories</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:048.10\" id=\"48.0\" class=\"lt\" onclick=\"jumptoTime(48, this.id)\"> from</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:048.46\" id=\"48.4\" class=\"lt\" onclick=\"jumptoTime(48, this.id)\"> most</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:048.80\" id=\"48.8\" class=\"lt\" onclick=\"jumptoTime(48, this.id)\"> obvious</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:049.22\" id=\"49.2\" class=\"lt\" onclick=\"jumptoTime(49, this.id)\"> to</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:049.68\" id=\"49.6\" class=\"lt\" onclick=\"jumptoTime(49, this.id)\"> least.</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:050.64\" id=\"50.6\" class=\"lt\" onclick=\"jumptoTime(50, this.id)\"> Small,</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:051.46\" id=\"51.4\" class=\"lt\" onclick=\"jumptoTime(51, this.id)\"> female,</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:052.66\" id=\"52.6\" class=\"lt\" onclick=\"jumptoTime(52, this.id)\"> incredible</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:053.44\" id=\"53.4\" class=\"lt\" onclick=\"jumptoTime(53, this.id)\"> fashion</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:054.02\" id=\"54.0\" class=\"lt\" onclick=\"jumptoTime(54, this.id)\"> sense,</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:055.36\" id=\"55.4\" class=\"lt\" onclick=\"jumptoTime(55, this.id)\"> and</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:056.08\" id=\"56.0\" class=\"lt\" onclick=\"jumptoTime(56, this.id)\"> so</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:056.44\" id=\"56.4\" class=\"lt\" onclick=\"jumptoTime(56, this.id)\"> on</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:056.70\" id=\"56.6\" class=\"lt\" onclick=\"jumptoTime(56, this.id)\"> and</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:056.86\" id=\"56.8\" class=\"lt\" onclick=\"jumptoTime(56, this.id)\"> so</a><!--\n",
            "\t\t\t\t--><a href=\"#00:00:057.04\" id=\"57.0\" class=\"lt\" onclick=\"jumptoTime(57, this.id)\"> forth.</a><!--\n",
            "\t\t\t\t--></p>\n",
            "</div>\n",
            "\t</body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#import webvtt\n",
        "import json\n",
        "from datetime import timedelta\n",
        "\n",
        "def timeStr(t):\n",
        "  return '{0:02d}:{1:02d}:{2:06.2f}'.format(round(t // 3600),\n",
        "                                                round(t % 3600 // 60),\n",
        "                                                t % 60)\n",
        "\n",
        "html = list(preS)\n",
        "txt = list(\"\")\n",
        "gidx = -1\n",
        "for g in groups:\n",
        "  shift = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
        "  shift = millisec(shift) - spacermilli #the start time in the original video\n",
        "  shift=max(shift, 0)\n",
        "\n",
        "  gidx += 1\n",
        "\n",
        "  captions = json.load(open(str(gidx) + '.json'))['segments']\n",
        "\n",
        "  if captions:\n",
        "    speaker = g[0].split()[-1]\n",
        "    boxclr = def_boxclr\n",
        "    spkrclr = def_spkrclr\n",
        "    if speaker in speakers:\n",
        "      speaker, boxclr, spkrclr = speakers[speaker]\n",
        "\n",
        "    html.append(f'<div class=\"e\" style=\"background-color: {boxclr}\">\\n');\n",
        "    html.append('<p  style=\"margin:0;padding: 5px 10px 10px 10px;word-wrap:normal;white-space:normal;\">\\n')\n",
        "    html.append(f'<span style=\"color:{spkrclr};font-weight: bold;\">{speaker}</span><br>\\n\\t\\t\\t\\t')\n",
        "\n",
        "    for c in captions:\n",
        "      start = shift + c['start'] * 1000.0\n",
        "      start = start / 1000.0   #time resolution ot youtube is Second.\n",
        "      end = (shift + c['end'] * 1000.0) / 1000.0\n",
        "      txt.append(f'[{timeStr(start)} --> {timeStr(end)}] [{speaker}] {c[\"text\"]}\\n')\n",
        "\n",
        "      for i, w in enumerate(c['words']):\n",
        "        if w == \"\":\n",
        "           continue\n",
        "        start = (shift + w['start']*1000.0) / 1000.0\n",
        "        #end = (shift + w['end']) / 1000.0   #time resolution ot youtube is Second.\n",
        "        html.append(f'<a href=\"#{timeStr(start)}\" id=\"{\"{:.1f}\".format(round(start*5)/5)}\" class=\"lt\" onclick=\"jumptoTime({int(start)}, this.id)\">{w[\"word\"]}</a><!--\\n\\t\\t\\t\\t-->')\n",
        "    #html.append('\\n')\n",
        "    html.append('</p>\\n')\n",
        "    html.append(f'</div>\\n')\n",
        "\n",
        "html.append(postS)\n",
        "\n",
        "\n",
        "with open(f\"capspeaker.txt\", \"w\", encoding='utf-8') as file:\n",
        "  s = \"\".join(txt)\n",
        "  file.write(s)\n",
        "  print('captions saved to capspeaker.txt:')\n",
        "  print(s+'\\n')\n",
        "\n",
        "with open(f\"capspeaker.html\", \"w\", encoding='utf-8') as file:    #TODO: proper html embed tag when video/audio from file\n",
        "  s = \"\".join(html)\n",
        "  file.write(s)\n",
        "  print('captions saved to capspeaker.html:')\n",
        "  print(s+'\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8yo4_nR8mv9b",
        "o-a6pLioVHjl",
        "1u1vbqd_VzNp"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}